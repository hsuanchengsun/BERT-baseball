{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation, LSTM, Dropout, TimeDistributed, Flatten\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_train_load = pd.read_csv('x_training_origin.csv', delimiter=',')\n",
    "y_train_load = pd.read_csv('y_training_origin.csv', delimiter=',')\n",
    "\n",
    "x_val_load = pd.read_csv('x_validation_origin.csv', delimiter=',')\n",
    "y_val_load = pd.read_csv('y_validation_origin.csv', delimiter=',')\n",
    "\n",
    "x_test_load = pd.read_csv('x_2018.csv', delimiter=',')\n",
    "y_test_load = pd.read_csv('y_2018.csv', delimiter=',')\n",
    "\n",
    "x_2019_load = pd.read_csv('x_2019.csv', delimiter=',')\n",
    "y_2019_load = pd.read_csv('y_2019.csv', delimiter=',')\n",
    "\n",
    "numTrainSize = x_train_load.shape[0]\n",
    "numValidatSize = x_val_load.shape[0]\n",
    "numTestSize = x_test_load.shape[0]\n",
    "num2019Size = x_2019_load.shape[0]\n",
    "numVar = x_train_load.shape[1]\n",
    "\n",
    "print(\"There are \" +  str(numTrainSize + numValidatSize + numTestSize) + \" training data with \" +  str(numVar) + \" variables, which equals to 4 data information and 21 variables of 1 year for total 5 continuous years.\")\n",
    "print(\"There are \" + str(num2019Size) + \" data used for prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['FirstYear_TB', 'SecondYear_TB', 'ThirdYear_TB', 'FourthYear_TB', 'FifthYear_TB']\n",
    "x_train_load.drop(columns = column, inplace = True)\n",
    "x_val_load.drop(columns = column, inplace = True)\n",
    "x_test_load.drop(columns = column, inplace = True)\n",
    "x_2019_load.drop(columns = column, inplace = True)\n",
    "\n",
    "x_train_sep = x_train_load.iloc[:,4:]\n",
    "x_val_sep = x_val_load.iloc[:,4:]\n",
    "x_test_sep = x_test_load\n",
    "x_2019_sep = x_2019_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating and normalizing input data\n",
    "\n",
    "x_train_norm = (x_train_sep - x_train_sep.min())  / (x_train_sep.max() - x_train_sep.min())\n",
    "x_val_norm = (x_val_sep - x_val_sep.min())  / (x_val_sep.max() - x_val_sep.min())\n",
    "x_test_norm = (x_test_sep - x_test_sep.min())  / (x_test_sep.max() - x_test_sep.min())\n",
    "x_2019_norm = (x_2019_sep - x_2019_sep.min())  / (x_2019_sep.max() - x_2019_sep.min())\n",
    "\n",
    "\n",
    "x_train_reshape = np.reshape(x_train_norm.values, (numTrainSize, 5, 20))\n",
    "x_val_reshape = np.reshape(x_val_norm.values, (numValidatSize, 5, 20))\n",
    "x_test_reshape = np.reshape(x_test_norm.values, (numTestSize, 5, 20))\n",
    "x_2019_reshape = np.reshape(x_2019_norm.values, (num2019Size, 5, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating output data\n",
    "\n",
    "y_train = y_train_load[y_train_load.columns[3:]]\n",
    "y_val = y_val_load[y_val_load.columns[3:]]\n",
    "y_test = y_test_load\n",
    "y_2019 = y_2019_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify\n",
    "\n",
    "y_train_TY = y_train.values\n",
    "for k in range(numTrainSize):\n",
    "    y_train_TY[k] = np.floor(y_train_TY[k]/5)# each interval is 5 HR\n",
    "\n",
    "y_val_TY = y_val.values\n",
    "for k in range(numValidatSize):\n",
    "    y_val_TY[k] = np.floor(y_val_TY[k]/5)# each interval is 5 HR\n",
    "    \n",
    "y_test_TY = y_test.values\n",
    "for k in range(numTestSize):\n",
    "    y_test_TY[k] = np.floor(y_test_TY[k]/5)# each interval is 5 HR\n",
    "    \n",
    "y_2019_TY = y_2019.values\n",
    "for k in range(num2019Size):\n",
    "    y_2019_TY[k] = np.floor(y_2019_TY[k]/5)# each interval is 5 HR  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train distribution\n",
    "\n",
    "bins = np.arange(0, 13,1)\n",
    "plt.hist(y_train_TY, bins = bins, alpha = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val distribution\n",
    "\n",
    "bins = np.arange(0, 13,1)\n",
    "plt.hist(y_val_TY, bins = bins, alpha = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test distribution\n",
    "\n",
    "bins = np.arange(0, 13,1)\n",
    "plt.hist(y_test_TY, bins = bins, alpha = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_2019 distribution\n",
    "\n",
    "bins = np.arange(0, 13,1)\n",
    "plt.hist(y_2019_TY, bins = bins, alpha = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot\n",
    "\n",
    "y_train_cat = np_utils.to_categorical(y_train_TY, 12)\n",
    "y_val_cat = np_utils.to_categorical(y_val_TY, 12)\n",
    "y_test_cat = np_utils.to_categorical(y_test_TY, 12)\n",
    "y_2019_cat = np_utils.to_categorical(y_2019_TY, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate train and val to be the training data\n",
    "\n",
    "x_train = np.concatenate((x_train_reshape, x_val_reshape))\n",
    "y_train = np.concatenate((y_train_cat, y_val_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "\n",
    "model_1 = load_model('model_1.h5')\n",
    "model_2 = load_model('model_2.h5')\n",
    "model_3 = load_model('model_3.h5')\n",
    "model_4 = load_model('model_4.h5')\n",
    "model_5 = load_model('model_5.h5')\n",
    "model_6 = load_model('model_6.h5')\n",
    "model_7 = load_model('model_7.h5')\n",
    "model_8 = load_model('model_8.h5')\n",
    "model_9 = load_model('model_9.h5')\n",
    "model_10 = load_model('model_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict 2019 classes\n",
    "\n",
    "result_2019_model_1 = model_1.predict_classes(x_2019_reshape)\n",
    "result_2019_model_2 = model_2.predict_classes(x_2019_reshape)\n",
    "result_2019_model_3= model_3.predict_classes(x_2019_reshape)\n",
    "result_2019_model_4 = model_4.predict_classes(x_2019_reshape)\n",
    "result_2019_model_5 = model_5.predict_classes(x_2019_reshape)\n",
    "result_2019_model_6 = model_6.predict_classes(x_2019_reshape)\n",
    "result_2019_model_7 = model_7.predict_classes(x_2019_reshape)\n",
    "result_2019_model_8 = model_8.predict_classes(x_2019_reshape)\n",
    "result_2019_model_9 = model_9.predict_classes(x_2019_reshape)\n",
    "result_2019_model_10 = model_10.predict_classes(x_2019_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1 confusion metrix\n",
    "\n",
    "pd.crosstab(y_2019_TY.reshape(-1), result_2019_model_1, rownames= [\"answer\"], colnames= [\"predict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "k = 0\n",
    "l = 0\n",
    "for i in range(len(y_2019_TY)):\n",
    "    if y_2019_TY[i] == result_2019_model_1[i]:\n",
    "        j += 1\n",
    "    elif y_2019_TY[i]+1 == result_2019_model_1[i]:\n",
    "        k += 1\n",
    "    elif y_2019_TY[i]-1 == result_2019_model_1[i]:\n",
    "        l += 1\n",
    "        \n",
    "print(\"準確率：%.4f\" % (j/len(y_2019_TY)))\n",
    "print(\"高估率：%.4f\" % (k/len(y_2019_TY)))\n",
    "print(\"低估率：%.4f\" % (l/len(y_2019_TY)))\n",
    "print(\"總和：%.4f\" % ((j+k+l)/len(y_2019_TY)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
